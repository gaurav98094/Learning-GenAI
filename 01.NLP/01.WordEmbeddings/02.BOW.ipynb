{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW\n",
    "\n",
    "The Bag of Words (BoW) model is a simple and widely used method for text representation in natural language processing (NLP). It represents text data as a collection of words (features) without considering grammar, word order, or context. This model is particularly useful for text classification, information retrieval, and other text-related tasks.\n",
    "\n",
    "<img src=\"images/bow.png\">\n",
    "\n",
    "### Layman Terms\n",
    "Imagine you have a bunch of documents, and you want to represent each document by the words it contains. The Bag of Words model creates a list of all unique words in your documents and then represents each document as a list of word counts. It doesn't care about the order of the words; it just counts how many times each word appears.\n",
    "\n",
    "### Example\n",
    "Let's say you have two sentences:\n",
    "1. \"The cat sits on the mat.\"\n",
    "2. \"The dog plays with the cat.\"\n",
    "\n",
    "The Bag of Words model would create a list of unique words: [\"the\", \"cat\", \"sits\", \"on\", \"mat\", \"dog\", \"plays\", \"with\"]\n",
    "\n",
    "Each document is then represented as a vector of word counts:\n",
    "- Sentence 1: [2, 1, 1, 1, 1, 0, 0, 0]\n",
    "- Sentence 2: [2, 1, 0, 0, 0, 1, 1, 1]\n",
    "\n",
    "### Math Behind It\n",
    "1. **Vocabulary**: Create a list of all unique words in the corpus.\n",
    "2. **Vector Representation**: For each document, create a vector where each element represents the count of a word in the document.\n",
    "\n",
    "### Example Code\n",
    "Here's an example of how to implement the Bag of Words model using the `sklearn` library in Python.\n",
    "\n",
    "### Installation\n",
    "First, you need to install the `scikit-learn` and `nltk` libraries if you haven't already:\n",
    "```bash\n",
    "pip install scikit-learn nltk\n",
    "```\n",
    "\n",
    "### Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gauravkandel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data files (only need to run once)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "documents = [\n",
    "    \"The cat sits on the mat\",\n",
    "    \"The dog plays with the cat\",\n",
    "    \"Dogs and cats are great pets\",\n",
    "    \"The mat is under the table\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the documents (tokenization)\n",
    "tokenized_documents = [\" \".join(word_tokenize(doc.lower())) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Count Vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "bow_matrix = vectorizer.fit_transform(tokenized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the BoW matrix to a dense format and display it\n",
    "dense_bow = bow_matrix.todense()\n",
    "bow_df = pd.DataFrame(dense_bow, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Matrix:\n",
      "   and  are  cat  cats  dog  dogs  great  is  mat  on  pets  plays  sits  \\\n",
      "0    0    0    1     0    0     0      0   0    1   1     0      0     1   \n",
      "1    0    0    1     0    1     0      0   0    0   0     0      1     0   \n",
      "2    1    1    0     1    0     1      1   0    0   0     1      0     0   \n",
      "3    0    0    0     0    0     0      0   1    1   0     0      0     0   \n",
      "\n",
      "   table  the  under  with  \n",
      "0      0    2      0     0  \n",
      "1      0    2      0     1  \n",
      "2      0    0      0     0  \n",
      "3      1    2      1     0  \n"
     ]
    }
   ],
   "source": [
    "print(\"Bag of Words Matrix:\")\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get the word count for a specific word in a specific document\n",
    "doc_index = 0  # First document\n",
    "word = 'cat'\n",
    "word_index = feature_names.tolist().index(word)\n",
    "word_count = bow_matrix[doc_index, word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for 'cat' in document 0: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word count for '{word}' in document {doc_index}: {word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "1. **Tokenize Documents**: We tokenize the documents using NLTK's `word_tokenize` and convert them to lowercase.\n",
    "2. **Count Vectorizer**: We initialize a `CountVectorizer` from `sklearn`.\n",
    "3. **Fit and Transform**: We fit the vectorizer to the tokenized documents and transform them into a Bag of Words matrix.\n",
    "4. **Feature Names**: We retrieve the feature names (words) from the vectorizer.\n",
    "5. **Dense Format**: We convert the sparse Bag of Words matrix to a dense format for easier display and create a DataFrame for better readability.\n",
    "6. **Word Count**: We get the word count for a specific word in a specific document.\n",
    "\n",
    "### Note:\n",
    "In a real-world scenario, you might need to preprocess the text further (e.g., removing stop words, stemming/lemmatization) and use a larger corpus of documents. The example above is simplified for clarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
