{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It is often used in text mining and information retrieval to identify the most significant words in documents.\n",
    "\n",
    "### Layman Terms\n",
    "Imagine you have a set of documents and you want to find out which words are the most important in each document. Simply counting the words (term frequency) isn't enough because common words like \"the\" and \"is\" will appear frequently in many documents but may not be important. TF-IDF helps by reducing the weight of common words and increasing the weight of words that are unique or less common in the corpus.\n",
    "\n",
    "### Math Behind It\n",
    "1. **Term Frequency (TF)**: Measures how frequently a term appears in a document. \n",
    "   - \\( \\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} \\)\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: Measures how important a term is. While computing TF, all terms are considered equally important. IDF reduces the weight of terms that occur very frequently in the corpus and increases the weight of terms that occur rarely.\n",
    "   - \\( \\text{IDF}(t, D) = \\log \\left( \\frac{\\text{Total number of documents } N}{\\text{Number of documents with term } t} \\right) \\)\n",
    "\n",
    "3. **TF-IDF**: Combines the two measures.\n",
    "   - \\( \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) \\)\n",
    "\n",
    "\n",
    "<img src=\"images/tfidf.png\">\n",
    "\n",
    "### Example Code\n",
    "Here's a simple implementation of TF-IDF using the `sklearn` library in Python.\n",
    "\n",
    "### Installation\n",
    "First, you need to install the `scikit-learn` and `nltk` libraries if you haven't already:\n",
    "```bash\n",
    "pip install scikit-learn nltk\n",
    "```\n",
    "\n",
    "### Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gauravkandel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data files (only need to run once)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "documents = [\n",
    "    \"The cat sits on the mat\",\n",
    "    \"The dog plays with the cat\",\n",
    "    \"Dogs and cats are great pets\",\n",
    "    \"The mat is under the table\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the documents (tokenization)\n",
    "tokenized_documents = [\" \".join(word_tokenize(doc.lower())) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(tokenized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TF-IDF matrix to a dense format and display it\n",
    "dense_tfidf = tfidf_matrix.todense()\n",
    "tfidf_df = pd.DataFrame(dense_tfidf, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "        and       are       cat      cats       dog      dogs     great  \\\n",
      "0  0.000000  0.000000  0.357160  0.000000  0.000000  0.000000  0.000000   \n",
      "1  0.000000  0.000000  0.344051  0.000000  0.436384  0.000000  0.000000   \n",
      "2  0.408248  0.408248  0.000000  0.408248  0.000000  0.408248  0.408248   \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "         is       mat        on      pets     plays      sits     table  \\\n",
      "0  0.000000  0.357160  0.453012  0.000000  0.000000  0.453012  0.000000   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.436384  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  0.408248  0.000000  0.000000  0.000000   \n",
      "3  0.436384  0.344051  0.000000  0.000000  0.000000  0.000000  0.436384   \n",
      "\n",
      "        the     under      with  \n",
      "0  0.578303  0.000000  0.000000  \n",
      "1  0.557077  0.000000  0.436384  \n",
      "2  0.000000  0.000000  0.000000  \n",
      "3  0.557077  0.436384  0.000000  \n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get the TF-IDF score for a specific word in a specific document\n",
    "doc_index = 0  # First document\n",
    "word = 'cat'\n",
    "word_index = feature_names.tolist().index(word)\n",
    "tfidf_score = tfidf_matrix[doc_index, word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF score for 'cat' in document 0: 0.35715970626521537\n"
     ]
    }
   ],
   "source": [
    "print(f\"TF-IDF score for '{word}' in document {doc_index}: {tfidf_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "1. **Tokenize Documents**: We tokenize the documents using NLTK's `word_tokenize` and convert them to lowercase.\n",
    "2. **TF-IDF Vectorizer**: We initialize a `TfidfVectorizer` from `sklearn`.\n",
    "3. **Fit and Transform**: We fit the vectorizer to the tokenized documents and transform them into a TF-IDF matrix.\n",
    "4. **Feature Names**: We retrieve the feature names (words) from the vectorizer.\n",
    "5. **Dense Format**: We convert the sparse TF-IDF matrix to a dense format for easier display and create a DataFrame for better readability.\n",
    "6. **TF-IDF Score**: We get the TF-IDF score for a specific word in a specific document.\n",
    "\n",
    "### Note:\n",
    "In a real-world scenario, you'd typically preprocess the text further (e.g., removing stop words, stemming/lemmatization) and use a larger corpus of documents. The example above is simplified for clarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
