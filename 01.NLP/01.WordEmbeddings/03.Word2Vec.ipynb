{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WOrd2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is a popular technique for natural language processing that represents words in a continuous vector space. It was developed by a team of researchers at Google led by Tomas Mikolov. The main idea behind Word2Vec is to map words into vectors of real numbers in such a way that words with similar meanings are close to each other in this vector space.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/word2vec2.png\">\n",
    "</center>\n",
    "\n",
    "Here's an explanation in layman terms and a bit of the math behind it:\n",
    "\n",
    "### Layman Terms\n",
    "Imagine you have a dictionary of words and you want to find a way to understand the relationship between these words. For example, you know that \"king\" is related to \"queen\" in a similar way that \"man\" is related to \"woman\". Word2Vec helps us find these relationships by representing each word as a point in a high-dimensional space (imagine a space with many directions, not just the usual 3D space we're familiar with).\n",
    "\n",
    "1. **Training on Context**: Word2Vec uses a large corpus of text to learn these relationships. It looks at words that appear close to each other (context) and tries to predict a word given its neighboring words.\n",
    "   \n",
    "2. **Vectors of Numbers**: Each word is represented by a list of numbers (a vector). Words that appear in similar contexts will have similar vectors. For instance, the vectors for \"king\" and \"queen\" will be close to each other in this space.\n",
    "\n",
    "3. **Finding Relationships**: Once trained, you can use these vectors to find relationships. For example, you can add and subtract vectors: \"king\" - \"man\" + \"woman\" should result in a vector close to \"queen\".\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"images/word2vec1.png\">\n",
    "</center>\n",
    "\n",
    "### Math Behind It\n",
    "Word2Vec uses two main approaches: Continuous Bag of Words (CBOW) and Skip-Gram.\n",
    "\n",
    "1. **CBOW**: Predicts the current word based on the context (neighboring words). \n",
    "   - For example, given the sentence \"The cat sits on the mat,\" CBOW would use the context (\"The\", \"sits\", \"on\", \"the\", \"mat\") to predict the word \"cat\".\n",
    "   \n",
    "2. **Skip-Gram**: Predicts the context based on the current word.\n",
    "   - Using the same sentence, Skip-Gram would use the word \"cat\" to predict the context words (\"The\", \"sits\", \"on\", \"the\", \"mat\").\n",
    "\n",
    "Both methods rely on neural networks to learn these relationships. Hereâ€™s a simplified overview of the math involved:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/word2vec3.png\">\n",
    "</center>\n",
    "\n",
    "\n",
    "1. **Input Layer**: For each word in the vocabulary, Word2Vec uses a one-hot encoded vector, which is a vector with all zeros except for a 1 at the position corresponding to that word.\n",
    "   \n",
    "2. **Hidden Layer**: This layer transforms the one-hot vector into a dense vector of lower dimensions. If the vocabulary size is \\( V \\) and the vector dimension is \\( N \\), the weight matrix between the input and hidden layer will be of size \\( V \\times N \\).\n",
    "\n",
    "3. **Output Layer**: For CBOW, the output layer uses the hidden layer vector to predict the context words. For Skip-Gram, it uses the hidden layer vector to predict each word in the context.\n",
    "\n",
    "4. **Training**: The network is trained using backpropagation to minimize the error in predicting context words (for Skip-Gram) or the center word (for CBOW). The optimization algorithm typically used is stochastic gradient descent (SGD).\n",
    "\n",
    "### Example:\n",
    "Imagine we have a small vocabulary of 5 words: [I, like, to, eat, apples]\n",
    "\n",
    "- One-hot vectors:\n",
    "  - \"I\": [1, 0, 0, 0, 0]\n",
    "  - \"like\": [0, 1, 0, 0, 0]\n",
    "  - \"to\": [0, 0, 1, 0, 0]\n",
    "  - \"eat\": [0, 0, 0, 1, 0]\n",
    "  - \"apples\": [0, 0, 0, 0, 1]\n",
    "\n",
    "- If the vector dimension \\( N \\) is 2, we might learn vectors like:\n",
    "  - \"I\": [0.1, 0.3]\n",
    "  - \"like\": [0.4, 0.2]\n",
    "  - \"to\": [0.3, 0.5]\n",
    "  - \"eat\": [0.6, 0.1]\n",
    "  - \"apples\": [0.7, 0.3]\n",
    "\n",
    "By training the model on a large corpus, these vectors will capture the semantic relationships between words.\n",
    "\n",
    "In summary, Word2Vec converts words into vectors where the distances and directions between vectors capture the semantic relationships between the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here's a simple implementation of Word2Vec using the popular `gensim` library in Python. This library makes it easy to train Word2Vec models.\n",
    "\n",
    "### Installation\n",
    "First, you need to install the `gensim` library if you haven't already:\n",
    "```bash\n",
    "pip install gensim\n",
    "```\n",
    "\n",
    "### Example Code\n",
    "Here's an example of how to train a Word2Vec model using a sample corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gauravkandel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "sentences = [\n",
    "    \"The cat sits on the mat\",\n",
    "    \"The dog plays with the cat\",\n",
    "    \"Dogs and cats are great pets\",\n",
    "    \"The mat is under the table\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'cat', 'sits', 'on', 'the', 'mat'],\n",
       " ['the', 'dog', 'plays', 'with', 'the', 'cat'],\n",
       " ['dogs', 'and', 'cats', 'are', 'great', 'pets'],\n",
       " ['the', 'mat', 'is', 'under', 'the', 'table']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the corpus\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=3, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'cat': [-0.01632538  0.00900339 -0.00828626  0.00163666  0.01699389 -0.00893211\n",
      "  0.00905091 -0.01355952 -0.00709418  0.01878481 -0.00314987  0.00065049\n",
      " -0.00826855 -0.01538342 -0.00300198  0.00493004 -0.00177135  0.01107892\n",
      " -0.00550515  0.0045054   0.01092333  0.01669503 -0.0028946  -0.01840428\n",
      "  0.00872835  0.00114171  0.01488347 -0.00161519 -0.00528901 -0.01750249\n",
      " -0.00171835  0.00565724  0.01080736  0.01410691 -0.01141086  0.00372608\n",
      "  0.01220445 -0.00959133 -0.00622664  0.01359332  0.00326536  0.00037815\n",
      "  0.00694771  0.00042967  0.01926994  0.01012401 -0.01783269 -0.01410048\n",
      "  0.0018009   0.01277822]\n"
     ]
    }
   ],
   "source": [
    "# Get the vector for a specific word\n",
    "cat_vector = model.wv['cat']\n",
    "print(\"Vector for 'cat':\", cat_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'cat': [('dogs', 0.22990256547927856), ('sits', 0.1249711662530899), ('are', 0.08071292191743851), ('dog', 0.07469211518764496), ('the', 0.04245809465646744)]\n"
     ]
    }
   ],
   "source": [
    "# Find similar words\n",
    "similar_words = model.wv.most_similar('cat', topn=5)\n",
    "print(\"Words similar to 'cat':\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Vector arithmetic: king - man + woman = ?\n",
    "# (Note: In this simple example, 'king' and 'queen' are not in the vocabulary, but this shows the general idea)\n",
    "# result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "# print(\"Result of 'king' - 'man' + 'woman':\", result)\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "1. **Tokenize Sentences**: We first tokenize the sentences in the corpus using NLTK's `word_tokenize`.\n",
    "2. **Train Word2Vec Model**: We train a Word2Vec model using `gensim`. Key parameters include:\n",
    "   - `vector_size`: The number of dimensions of the word vectors.\n",
    "   - `window`: The maximum distance between the current and predicted word within a sentence.\n",
    "   - `min_count`: Ignores all words with a total frequency lower than this.\n",
    "   - `sg`: Defines the training algorithm. 1 for Skip-Gram; 0 for CBOW.\n",
    "3. **Save and Load Model**: The model can be saved and loaded for later use.\n",
    "4. **Get Word Vectors**: Retrieve the vector for a specific word.\n",
    "5. **Find Similar Words**: Use the model to find words similar to a given word.\n",
    "\n",
    "### Note:\n",
    "In a real-world scenario, you'd use a much larger corpus of text to train the model, and the `vector_size` would typically be higher (e.g., 100 or 300). The example above is simplified for clarity.\n",
    "\n",
    "<img src=\"images/image.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
